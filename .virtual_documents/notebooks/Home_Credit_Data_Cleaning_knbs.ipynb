



import os, io, json, math, zipfile, warnings, re
from pathlib import Path

import numpy as np
import pandas as pd
import requests

from sklearn import __version__ as sklver
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

warnings.filterwarnings('ignore')
pd.set_option('display.max_columns', 120)

DATA_DIR = Path('data')
KNBS_DIR = DATA_DIR / 'knbs'
OUTPUTS_DIR = DATA_DIR / 'outputs'
TRAIN_PATH = DATA_DIR / 'application_train.csv'
FINAL_OUTPUT = OUTPUTS_DIR / 'final_dataset_kenya.csv'

WB_COUNTRY = 'KE'
WB_INDICATORS = {
    'ke_inflation_annual_pct': 'FP.CPI.TOTL.ZG',
    'ke_gdp_growth_pct': 'NY.GDP.MKTP.KD.ZG',
    'ke_unemployment_pct': 'SL.UEM.TOTL.ZS',
}

def enc_kwargs():
    minor = int(sklver.split('.')[1])
    return dict(sparse_output=False) if minor >= 2 else dict(sparse=False)

def norm_county(s: str) -> str:
    if pd.isna(s): return s
    s = str(s).strip().lower()
    s = re.sub(r'\s+', ' ', s)
    s = s.replace('-', ' ')
    return s






df = pd.read_csv("../data/raw/application_train.csv")
print('Raw shape:', df.shape)
display(df.head(3))







# Standardize columns
df.columns = [c.strip().lower().replace(' ', '_') for c in df.columns]

# Remove duplicates
before = len(df)
df = df.drop_duplicates()
print('Removed dups:', before - len(df))

# Target
if 'target' not in df.columns:
    raise ValueError('Expected target column `TARGET` not found.')
y = df['target'].copy()
X = df.drop(columns=['target'])

# Feature engineering
def safe_div(a, b):
    return np.where(b==0, np.nan, a / b)

if {'amt_credit','amt_income_total'} <= set(X.columns):
    X['credit_income_percent'] = safe_div(X['amt_credit'], X['amt_income_total'])
if {'amt_annuity','amt_income_total'} <= set(X.columns):
    X['annuity_income_percent'] = safe_div(X['amt_annuity'], X['amt_income_total'])
if {'days_employed','days_birth'} <= set(X.columns):
    emp_years = (-X['days_employed']) / 365.25
    age_years = (-X['days_birth']) / 365.25
    X['employment_to_age'] = safe_div(emp_years, age_years)

for col in ['amt_income_total','amt_credit','amt_annuity']:
    if col in X.columns:
        X[f'{col}_missing'] = X[col].isna().astype(int)

# Detect applicant_county if present; normalize for merging later
county_col = None
for c in X.columns:
    if c.lower() == 'applicant_county':
        county_col = c
        break
if county_col:
    X['applicant_county_norm'] = X[county_col].map(norm_county)
    print('Detected applicant_county column for county-level merge.')

print('Current shape:', X.shape)







numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
categorical_cols = [c for c in X.columns if c not in numeric_cols]

cat_args = enc_kwargs()

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler(with_mean=True))
])
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', **cat_args))
])

preprocess = ColumnTransformer(
    transformers=[('num', numeric_transformer, numeric_cols),
                  ('cat', categorical_transformer, categorical_cols)],
    sparse_threshold=0.0
)

preprocess.fit(X)
Xp = preprocess.transform(X)  # dense ndarray
print('Transformed shape:', Xp.shape)

num_features = numeric_cols
cat_features = list(preprocess.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_cols)) if categorical_cols else []
feature_names = num_features + cat_features

Xp_df = pd.DataFrame(Xp, columns=feature_names)
Xp_df['target'] = y.values
Xp_df.index = df.index  # align with original rows
print('Feature frame:', Xp_df.shape)







def fetch_worldbank_indicator(country_code: str, indicator_code: str) -> pd.DataFrame:
    url_xls = f'https://api.worldbank.org/v2/country/{country_code}/indicator/{indicator_code}?downloadformat=excel'
    try:
        r = requests.get(url_xls, timeout=60)
        r.raise_for_status()
        z = zipfile.ZipFile(io.BytesIO(r.content))
        xls_name = [n for n in z.namelist() if n.lower().endswith(('.xls','.xlsx'))][0]
        df_ind = pd.read_excel(z.open(xls_name), sheet_name=0)
        year_cols = [c for c in df_ind.columns if (isinstance(c, (int,float)) or (isinstance(c,str) and c.isdigit()))]
        tidy = df_ind.melt(id_vars=[c for c in df_ind.columns if c not in year_cols],
                           value_vars=year_cols, var_name='year', value_name='value')
        tidy['year'] = pd.to_numeric(tidy['year'], errors='coerce')
        return tidy[['year','value']].dropna().sort_values('year').reset_index(drop=True)
    except Exception:
        url_json = f'https://api.worldbank.org/v2/country/{country_code}/indicator/{indicator_code}?format=json&per_page=20000'
        rj = requests.get(url_json, timeout=60)
        rj.raise_for_status()
        data = rj.json()
        series = data[1] if isinstance(data, list) and len(data) > 1 else []
        rows = []
        for item in series:
            try:
                year = int(item.get('date'))
                val = item.get('value', None)
                if val is not None:
                    rows.append((year, float(val)))
            except Exception:
                pass
        return pd.DataFrame(rows, columns=['year','value']).sort_values('year').reset_index(drop=True)

wb_frames = {}
for colname, ind in WB_INDICATORS.items():
    try:
        t = fetch_worldbank_indicator(WB_COUNTRY, ind)
        wb_frames[colname] = t
        print(f'WB fetched: {colname} â€” {len(t)} rows; latest:', t.tail(1).to_dict('records'))
    except Exception as e:
        print(f'WB fetch failed for {colname}:', e)
        wb_frames[colname] = pd.DataFrame({'year': [], 'value': []})

latest_macro = {k: (float(v.dropna().iloc[-1]['value']) if len(v) else np.nan) for k, v in wb_frames.items()}
for k, v in latest_macro.items():
    Xp_df[k] = v
print('Added national macro snapshot:', latest_macro)







# Attempt to download published CSVs if available; otherwise, fallback to local CSVs placed under data/knbs/.
# Because KNBS often publishes as Excel/PDF, local CSVs are the more reliable route.
# We'll read any of the following if present:
#   - gcp_by_county.csv                 -> columns: county, year, gcp_current_prices_ksh
#   - census_2019_county.csv            -> columns: county, population_2019
#   - household_size_county.csv         -> columns: county, household_size_2019
#   - mobile_money_penetration_county.csv (optional) -> columns: county, mobile_money_penetration_pct

def read_if_exists(path, required_cols):
    if path.exists():
        dfc = pd.read_csv(path)
        # standardize headers
        dfc.columns = [c.strip().lower().replace(' ', '_') for c in dfc.columns]
        missing = [c for c in required_cols if c not in dfc.columns]
        if missing:
            print(f'Warning: {path.name} missing required columns: {missing}; skipping file.')
            return None
        return dfc
    return None

gcp_csv = KNBS_DIR / 'gcp_by_county.csv'
pop_csv = KNBS_DIR / 'census_2019_county.csv'
hh_csv  = KNBS_DIR / 'household_size_county.csv'
mm_csv  = KNBS_DIR / 'mobile_money_penetration_county.csv'

gcp_df = read_if_exists(gcp_csv, ['county','year','gcp_current_prices_ksh'])
pop_df = read_if_exists(pop_csv, ['county','population_2019'])
hh_df  = read_if_exists(hh_csv,  ['county','household_size_2019'])
mm_df  = read_if_exists(mm_csv,  ['county','mobile_money_penetration_pct'])

# Reduce GCP to most recent year per county and compute per-capita using population if available
if gcp_df is not None:
    gcp_df['county_norm'] = gcp_df['county'].map(norm_county)
    gcp_recent = gcp_df.sort_values(['county_norm','year']).groupby('county_norm', as_index=False).tail(1)
    gcp_recent = gcp_recent[['county_norm','gcp_current_prices_ksh','year']].rename(
        columns={'gcp_current_prices_ksh':'gcp_current_ksh_latest','year':'gcp_year_latest'})
else:
    gcp_recent = None

if pop_df is not None:
    pop_df['county_norm'] = pop_df['county'].map(norm_county)
    pop_recent = pop_df[['county_norm','population_2019']].copy()
else:
    pop_recent = None

if hh_df is not None:
    hh_df['county_norm'] = hh_df['county'].map(norm_county)
    hh_recent = hh_df[['county_norm','household_size_2019']].copy()
else:
    hh_recent = None

if mm_df is not None:
    mm_df['county_norm'] = mm_df['county'].map(norm_county)
    mm_recent = mm_df[['county_norm','mobile_money_penetration_pct']].copy()
else:
    mm_recent = None

# Combine the county indicators into one frame
county_feat = None
for piece in [gcp_recent, pop_recent, hh_recent, mm_recent]:
    if piece is None: 
        continue
    county_feat = piece if county_feat is None else county_feat.merge(piece, on='county_norm', how='outer')

# Compute GCP per capita if both GCP and population available
if county_feat is not None and {'gcp_current_ksh_latest','population_2019'} <= set(county_feat.columns):
    county_feat['gcp_per_capita_ksh'] = county_feat['gcp_current_ksh_latest'] / county_feat['population_2019']

if county_feat is not None:
    print('County features shape:', county_feat.shape)
    display(county_feat.head())
else:
    print('No county CSVs found under data/knbs/. Proceeding with national-only features.')







if 'applicant_county_norm' in X.columns and county_feat is not None:
    left = X[['applicant_county_norm']].copy()
    merged = left.merge(county_feat, left_on='applicant_county_norm', right_on='county_norm', how='left')
    # attach to feature frame by original index alignment
    for col in [c for c in merged.columns if c not in ['applicant_county_norm','county_norm']]:
        Xp_df[col] = merged[col].values
    print('Merged county features onto dataset.')
else:
    if 'applicant_county_norm' not in X.columns:
        print('County merge skipped: no `applicant_county` column detected in source data.')
    if county_feat is None:
        print('County merge skipped: no county features available (provide CSVs in data/knbs/).')







print('Any NA in target? ', Xp_df['target'].isna().any())
print('Overall NA ratio:', float(pd.isna(Xp_df).mean().mean()))

OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)
Xp_df.to_csv(FINAL_OUTPUT, index=False)
print('Saved final dataset to:', FINAL_OUTPUT.resolve())

display(Xp_df.sample(min(5, len(Xp_df))))




