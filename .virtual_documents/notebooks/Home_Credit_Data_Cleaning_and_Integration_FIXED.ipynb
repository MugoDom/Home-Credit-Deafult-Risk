



import os, io, json, math, zipfile, warnings
from pathlib import Path

import numpy as np
import pandas as pd
import requests

from sklearn import __version__ as sklver
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

warnings.filterwarnings('ignore')
pd.set_option('display.max_columns', 100)

def _enc_kwargs():
    # Version-safe kwargs for OneHotEncoder
    minor = int(sklver.split('.')[1])
    if minor >= 2:  # sklearn >= 1.2
        return dict(sparse_output=False)  # force dense
    else:
        return dict(sparse=False)         # force dense for older versions




# Paths
DATA_DIR = Path('data')
OUTPUT_DIR = Path('data/outputs')
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

TRAIN_PATH = DATA_DIR / 'application_train.csv'
FINAL_OUTPUT = OUTPUT_DIR / 'final_dataset_kenya.csv'

# World Bank indicators (Kenya)
WB_COUNTRY = 'KE'
WB_INDICATORS = {
    'ke_inflation_annual_pct': 'FP.CPI.TOTL.ZG',
    'ke_gdp_growth_pct': 'NY.GDP.MKTP.KD.ZG',
    'ke_unemployment_pct': 'SL.UEM.TOTL.ZS',
}







df = pd.read_csv("../data/raw/application_train.csv")
print('Shape:', df.shape)
display(df.head(3))
display(df.describe(include='all').T.head(20))







df.columns = [c.strip().lower().replace(' ', '_') for c in df.columns]

before = len(df)
df = df.drop_duplicates()
after = len(df)
print(f'Removed {before - after} duplicate rows')

target_col = 'target' if 'target' in df.columns else None
if target_col is None:
    raise ValueError('Expected target column `TARGET` not found.')

y = df[target_col].copy()
X = df.drop(columns=[target_col])







def safe_div(a, b):
    return np.where(b==0, np.nan, a / b)

if 'amt_credit' in X.columns and 'amt_income_total' in X.columns:
    X['credit_income_percent'] = safe_div(X['amt_credit'], X['amt_income_total'])

if 'amt_annuity' in X.columns and 'amt_income_total' in X.columns:
    X['annuity_income_percent'] = safe_div(X['amt_annuity'], X['amt_income_total'])

if 'days_employed' in X.columns and 'days_birth' in X.columns:
    emp_years = (-X['days_employed']) / 365.25
    age_years = (-X['days_birth']) / 365.25
    X['employment_to_age'] = safe_div(emp_years, age_years)

for col in ['amt_income_total','amt_credit','amt_annuity']:
    if col in X.columns:
        X[f'{col}_missing'] = X[col].isna().astype(int)

print('Engineered columns:', [c for c in X.columns if c.endswith('_percent') or c=='employment_to_age'])







numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
categorical_cols = [c for c in X.columns if c not in numeric_cols]
print('Numeric columns:', len(numeric_cols))
print('Categorical columns:', len(categorical_cols))







enc_kwargs = _enc_kwargs()

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler(with_mean=True))  # dense pipeline
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', **enc_kwargs))  # dense one-hot
])

preprocess = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_cols),
        ('cat', categorical_transformer, categorical_cols),
    ],
    sparse_threshold=0.0  # force dense output
)

preprocess.fit(X)
Xp = preprocess.transform(X)  # numpy ndarray (dense)
print('Transformed shape (dense):', Xp.shape)

num_features = numeric_cols
cat_features = []
if categorical_cols:
    cat_features = list(preprocess.named_transformers_['cat']
                        .named_steps['onehot']
                        .get_feature_names_out(categorical_cols))

feature_names = num_features + cat_features

Xp_df = pd.DataFrame(Xp, columns=feature_names)
Xp_df['target'] = y.values
print('Final feature frame shape:', Xp_df.shape)







import io, zipfile

def fetch_worldbank_indicator(country_code: str, indicator_code: str) -> pd.DataFrame:
    url = f'https://api.worldbank.org/v2/country/{country_code}/indicator/{indicator_code}?downloadformat=excel'
    r = requests.get(url, timeout=60)
    r.raise_for_status()
    z = zipfile.ZipFile(io.BytesIO(r.content))
    xls_name = [n for n in z.namelist() if n.lower().endswith(('.xls', '.xlsx'))][0]
    df_ind = pd.read_excel(z.open(xls_name), sheet_name=0, header=0)
    year_cols = [c for c in df_ind.columns if isinstance(c, (int, float)) or (isinstance(c, str) and c.isdigit())]
    tidy = df_ind.melt(id_vars=[c for c in df_ind.columns if c not in year_cols],
                       value_vars=year_cols, var_name='year', value_name='value')
    tidy['year'] = pd.to_numeric(tidy['year'], errors='coerce')
    tidy = tidy[['year','value']].dropna().sort_values('year')
    return tidy

wb_frames = {}
for colname, ind in WB_INDICATORS.items():
    try:
        series_df = fetch_worldbank_indicator(WB_COUNTRY, ind)
        wb_frames[colname] = series_df
        print(f'Fetched {colname} ({ind}) with {len(series_df)} rows.')
    except Exception as e:
        print(f'Fetch failed for {colname} ({ind}):', e)
        wb_frames[colname] = pd.DataFrame({'year': [], 'value': []})







latest_values = {}
for colname, df_ind in wb_frames.items():
    if len(df_ind):
        latest_row = df_ind.dropna().sort_values('year').iloc[-1]
        latest_values[colname] = float(latest_row['value'])
    else:
        latest_values[colname] = np.nan

print('Latest macro snapshot:', latest_values)

for k, v in latest_values.items():
    Xp_df[k] = v

# Optional year-wise merge template (if you derive per-row application_year):
# year_merge = None
# for k, df_ind in wb_frames.items():
#     t = df_ind.rename(columns={'value': k})
#     year_merge = t if year_merge is None else year_merge.merge(t, on='year', how='outer')
# Xp_df = Xp_df.merge(X[['application_year']], left_index=True, right_index=True, how='left')\
#              .merge(year_merge, left_on='application_year', right_on='year', how='left')\
#              .drop(columns=['year'])







print('Any NA in target? ', Xp_df['target'].isna().any())
print('Overall NA ratio:', float(Xp_df.isna().mean().mean()))

Xp_df.to_csv(FINAL_OUTPUT, index=False)
print('Saved final dataset to:', FINAL_OUTPUT.resolve())

Xp_df.sample(min(5, len(Xp_df)))




