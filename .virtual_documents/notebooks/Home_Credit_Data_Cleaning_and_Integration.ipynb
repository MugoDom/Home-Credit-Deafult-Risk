



import os
import io
import json
import math
import zipfile
import warnings
from pathlib import Path

import numpy as np
import pandas as pd
import requests

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

warnings.filterwarnings('ignore')
pd.set_option('display.max_columns', 100)




# === Paths ===
DATA_DIR = Path('data')          # change if needed
OUTPUT_DIR = Path('data/outputs')
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

TRAIN_PATH = DATA_DIR / 'application_train.csv'   # ensure this exists
FINAL_OUTPUT = OUTPUT_DIR / 'final_dataset_kenya.csv'

# === World Bank indicators (Kenya) ===
# Common indicator codes:
# - Inflation, consumer prices (annual %) .............. FP.CPI.TOTL.ZG
# - GDP growth (annual %) .............................. NY.GDP.MKTP.KD.ZG
# - Unemployment, total (% of total labor force) ....... SL.UEM.TOTL.ZS
WB_COUNTRY = 'KE'
WB_INDICATORS = {
    'ke_inflation_annual_pct': 'FP.CPI.TOTL.ZG',
    'ke_gdp_growth_pct': 'NY.GDP.MKTP.KD.ZG',
    'ke_unemployment_pct': 'SL.UEM.TOTL.ZS',
}







assert TRAIN_PATH.exists(), f"Train file not found at {TRAIN_PATH}. Place application_train.csv under {DATA_DIR}/"

df = pd.read_csv(TRAIN_PATH)
print('Shape:', df.shape)
display(df.head(3))
display(df.describe(include='all').T.head(20))







# Standardize column names
df.columns = [c.strip().lower().replace(' ', '_') for c in df.columns]

# Remove exact duplicate rows if any
before = len(df)
df = df.drop_duplicates()
after = len(df)
print(f'Removed {before - after} duplicate rows')

# Identify target column
target_col = 'target' if 'target' in df.columns else None
if target_col is None:
    raise ValueError('Expected target column `TARGET` not found.')

# Separate features and target
y = df[target_col].copy()
X = df.drop(columns=[target_col])







def safe_div(a, b):
    return np.where(b==0, np.nan, a / b)

# Known important columns in Home Credit
num_cols_present = [c for c in ['amt_credit','amt_annuity','amt_income_total','amt_goods_price',
                                'days_employed','days_birth','ext_source_1','ext_source_2','ext_source_3']
                   if c in X.columns]

if 'amt_credit' in X.columns and 'amt_income_total' in X.columns:
    X['credit_income_percent'] = safe_div(X['amt_credit'], X['amt_income_total'])

if 'amt_annuity' in X.columns and 'amt_income_total' in X.columns:
    X['annuity_income_percent'] = safe_div(X['amt_annuity'], X['amt_income_total'])

if 'days_employed' in X.columns and 'days_birth' in X.columns:
    emp_years = (-X['days_employed']) / 365.25
    age_years = (-X['days_birth']) / 365.25
    X['employment_to_age'] = safe_div(emp_years, age_years)

for col in ['amt_income_total','amt_credit','amt_annuity']:
    if col in X.columns:
        X[f'{col}_missing'] = X[col].isna().astype(int)

print('Engineered columns added:', [c for c in X.columns if c.endswith('_percent') or c=='employment_to_age'])







numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
categorical_cols = [c for c in X.columns if c not in numeric_cols]

print('Numeric columns:', len(numeric_cols))
print('Categorical columns:', len(categorical_cols))







numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler(with_mean=False))
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=True))
])

preprocess = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_cols),
        ('cat', categorical_transformer, categorical_cols),
    ]
)

preprocess.fit(X)
Xp = preprocess.transform(X)
print('Transformed shape:', Xp.shape)

num_features = numeric_cols
cat_features = []
if categorical_cols:
    cat_features = list(preprocess.named_transformers_['cat']
                        .named_steps['onehot']
                        .get_feature_names_out(categorical_cols))

feature_names = num_features + cat_features
Xp_df = pd.DataFrame.sparse.from_spmatrix(Xp, columns=feature_names)
Xp_df.reset_index(drop=True, inplace=True)
Xp_df['target'] = y.values
print('Final feature frame shape:', Xp_df.shape)







import io, zipfile, requests, pandas as pd

def fetch_worldbank_indicator(country_code: str, indicator_code: str) -> pd.DataFrame:
    url = f'https://api.worldbank.org/v2/country/{country_code}/indicator/{indicator_code}?downloadformat=excel'
    r = requests.get(url, timeout=60)
    r.raise_for_status()
    z = zipfile.ZipFile(io.BytesIO(r.content))
    xls_name = [n for n in z.namelist() if n.lower().endswith(('.xls', '.xlsx'))][0]
    df_ind = pd.read_excel(z.open(xls_name), sheet_name=0, header=0)
    year_cols = [c for c in df_ind.columns if isinstance(c, (int, float)) or (isinstance(c, str) and c.isdigit())]
    tidy = df_ind.melt(id_vars=[c for c in df_ind.columns if c not in year_cols],
                       value_vars=year_cols, var_name='year', value_name='value')
    tidy['year'] = pd.to_numeric(tidy['year'], errors='coerce')
    tidy = tidy[['year','value']].dropna().sort_values('year')
    return tidy

wb_frames = {}
for colname, ind in WB_INDICATORS.items():
    try:
        series_df = fetch_worldbank_indicator(WB_COUNTRY, ind)
        wb_frames[colname] = series_df
        print(f'Fetched {colname} ({ind}) with {len(series_df)} rows.')
    except Exception as e:
        print(f'Fetch failed for {colname} ({ind}):', e)
        wb_frames[colname] = pd.DataFrame({'year': [], 'value': []})







latest_values = {}
for colname, df_ind in wb_frames.items():
    if len(df_ind):
        latest_row = df_ind.dropna().sort_values('year').iloc[-1]
        latest_values[colname] = float(latest_row['value'])
    else:
        latest_values[colname] = np.nan

print('Latest macro snapshot:', latest_values)

for k, v in latest_values.items():
    Xp_df[k] = v

# (Optional) If/when you derive an application year, prefer a year-wise merge as shown below:
# Example:
# application_year = 2018  # placeholder
# year_merge = None
# for k, df_ind in wb_frames.items():
#     t = df_ind.rename(columns={'value': k})
#     year_merge = t if year_merge is None else year_merge.merge(t, on='year', how='outer')
# # Xp_df = Xp_df.merge(year_merge, left_on='application_year', right_on='year', how='left')







print('Any NA in target? ', Xp_df['target'].isna().any())
print('Overall NA ratio:', float(Xp_df.isna().mean().mean()))

Xp_df.to_csv(FINAL_OUTPUT, index=False)
print('Saved final dataset to:', FINAL_OUTPUT.resolve())

Xp_df.sample(min(5, len(Xp_df)))

