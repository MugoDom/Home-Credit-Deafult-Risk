{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73426961",
   "metadata": {},
   "source": [
    "# Export the dataset\n",
    "\n",
    "This step preprocesses the data for modelling by:\n",
    "  - Median imputation for numerical variables\n",
    "  - Mode imputation and one-hot encoding categorical variables\n",
    "  - Aligning the train/test columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0734ef00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /Users/domnjue/Desktop/Data-Science/projects/home_credit_default_risk\n",
      "CLEAN_DIR: /Users/domnjue/Desktop/Data-Science/projects/home_credit_default_risk/data/clean\n",
      "MODEL_DIR: /Users/domnjue/Desktop/Data-Science/projects/home_credit_default_risk/model\n"
     ]
    }
   ],
   "source": [
    "# Imports & configuration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "# Directories\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "RAW_DIR   = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "CLEAN_DIR = PROJECT_ROOT / \"data\" / \"clean\"\n",
    "MODEL_DIR = PROJECT_ROOT / \"model\"\n",
    "\n",
    "CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"CLEAN_DIR:\", CLEAN_DIR)\n",
    "print(\"MODEL_DIR:\", MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa45e8c",
   "metadata": {},
   "source": [
    "### 1. Separate target and IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "461eb152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train_merged: (307511, 181)\n",
      "Loaded test_merged: (48744, 180)\n"
     ]
    }
   ],
   "source": [
    "# Load merged application + bureau + behavior datasets\n",
    "train_merged = pd.read_csv(CLEAN_DIR / \"train_merged.csv\")\n",
    "test_merged  = pd.read_csv(CLEAN_DIR / \"test_merged.csv\")\n",
    "\n",
    "print(\"Loaded train_merged:\", train_merged.shape)\n",
    "print(\"Loaded test_merged:\", test_merged.shape)\n",
    "\n",
    "# Replace infinities with NaN (for safety)\n",
    "train_merged = train_merged.replace([np.inf, -np.inf], np.nan)\n",
    "test_merged  = test_merged.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "def clip_extreme_values(df: pd.DataFrame, max_abs: float = 1e12) -> pd.DataFrame:\n",
    "    \"\"\"Avoid insane values from blowing up the model.\"\"\"\n",
    "    df = df.copy()\n",
    "    num_cols_local = df.select_dtypes(include=[np.number]).columns\n",
    "    df[num_cols_local] = df[num_cols_local].clip(lower=-max_abs, upper=max_abs)\n",
    "    return df\n",
    "\n",
    "train_merged = clip_extreme_values(train_merged)\n",
    "test_merged  = clip_extreme_values(test_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f676d1",
   "metadata": {},
   "source": [
    "### 2. Identify categorical vs numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b64fb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_raw: (307511, 180)\n",
      "X_test_raw: (48744, 180)\n"
     ]
    }
   ],
   "source": [
    "# Separate target, ID, and raw features\n",
    "TARGET_COL = \"TARGET\"\n",
    "ID_COL = \"SK_ID_CURR\"\n",
    "\n",
    "# Target\n",
    "y = train_merged[TARGET_COL].astype(int)\n",
    "\n",
    "# Save IDs\n",
    "id_train = train_merged[ID_COL].values\n",
    "id_test  = test_merged[ID_COL].values\n",
    "\n",
    "# Raw feature frames (keep ID for now; drop TARGET from train)\n",
    "X_train_raw = train_merged.drop(columns=[TARGET_COL])\n",
    "X_test_raw  = test_merged.copy()\n",
    "\n",
    "print(\"X_train_raw:\", X_train_raw.shape)\n",
    "print(\"X_test_raw:\", X_test_raw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf7a0aa",
   "metadata": {},
   "source": [
    "### 3. Build the preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03bdd4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total feature cols: 179\n",
      "Number of categorical cols: 14\n",
      "Number of numeric cols: 165\n"
     ]
    }
   ],
   "source": [
    "# All features except ID\n",
    "feature_cols = [c for c in X_train_raw.columns if c != ID_COL]\n",
    "\n",
    "cat_cols = (\n",
    "    X_train_raw[feature_cols]\n",
    "    .select_dtypes(include=[\"object\", \"category\"])\n",
    "    .columns\n",
    "    .tolist()\n",
    ")\n",
    "num_cols = (\n",
    "    X_train_raw[feature_cols]\n",
    "    .select_dtypes(include=[np.number])\n",
    "    .columns\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "print(\"Total feature cols:\", len(feature_cols))\n",
    "print(\"Number of categorical cols:\", len(cat_cols))\n",
    "print(\"Number of numeric cols:\", len(num_cols))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9e8165",
   "metadata": {},
   "source": [
    "### 4. Fit on train only, the transform both train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4095f9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ColumnTransformer(transformers=[('num', SimpleImputer(strategy='median'),\n",
      "                                 ['FLAG_OWN_CAR', 'FLAG_OWN_REALTY',\n",
      "                                  'CNT_CHILDREN', 'AMT_INCOME_TOTAL',\n",
      "                                  'AMT_CREDIT', 'AMT_ANNUITY',\n",
      "                                  'AMT_GOODS_PRICE',\n",
      "                                  'REGION_POPULATION_RELATIVE', 'DAYS_BIRTH',\n",
      "                                  'DAYS_EMPLOYED', 'DAYS_REGISTRATION',\n",
      "                                  'DAYS_ID_PUBLISH', 'OWN_CAR_AGE',\n",
      "                                  'FLAG_MOBIL', 'FLAG_EMP_PHONE',\n",
      "                                  'FLAG_WORK_PHONE', 'FLAG_C...\n",
      "                                                 ('encoder',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore',\n",
      "                                                                sparse_output=False))]),\n",
      "                                 ['NAME_CONTRACT_TYPE', 'CODE_GENDER',\n",
      "                                  'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE',\n",
      "                                  'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS',\n",
      "                                  'NAME_HOUSING_TYPE', 'OCCUPATION_TYPE',\n",
      "                                  'WEEKDAY_APPR_PROCESS_START',\n",
      "                                  'ORGANIZATION_TYPE', 'FONDKAPREMONT_MODE',\n",
      "                                  'HOUSETYPE_MODE', 'WALLSMATERIAL_MODE',\n",
      "                                  'EMERGENCYSTATE_MODE'])])\n"
     ]
    }
   ],
   "source": [
    "# Build preprocessing pipeline (imputation + one-hot encoding)\n",
    "numeric_transformer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_cols),\n",
    "        (\"cat\", categorical_transformer, cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",  # ignore any unexpected cols\n",
    ")\n",
    "\n",
    "print(preprocessor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ea893e",
   "metadata": {},
   "source": [
    "### 5. Build feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee9edf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed train shape: (307511, 301)\n",
      "Transformed test shape: (48744, 301)\n"
     ]
    }
   ],
   "source": [
    "# Fit preprocessor on train, transform both train & test\n",
    "X_train_arr = preprocessor.fit_transform(X_train_raw[feature_cols])\n",
    "X_test_arr  = preprocessor.transform(X_test_raw[feature_cols])\n",
    "\n",
    "print(\"Transformed train shape:\", X_train_arr.shape)\n",
    "print(\"Transformed test shape:\", X_test_arr.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4884b4b",
   "metadata": {},
   "source": [
    "### 6. Wrap back into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f6fc4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of final features: 301\n",
      "Feature count matches train array? True\n"
     ]
    }
   ],
   "source": [
    "# Extract final feature names\n",
    "\n",
    "num_feature_names = num_cols\n",
    "\n",
    "# One-hot feature names for categorical columns\n",
    "ohe = preprocessor.named_transformers_[\"cat\"].named_steps[\"encoder\"]\n",
    "ohe_feature_names = ohe.get_feature_names_out(cat_cols).tolist()\n",
    "\n",
    "final_feature_names = num_feature_names + ohe_feature_names\n",
    "\n",
    "print(\"Number of final features:\", len(final_feature_names))\n",
    "print(\n",
    "    \"Feature count matches train array?\",\n",
    "    len(final_feature_names) == X_train_arr.shape[1],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84c570d",
   "metadata": {},
   "source": [
    "## 7. Export final modeling datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdbaf685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_model shape: (307511, 303)\n",
      "X_test_model shape: (48744, 302)\n",
      "Train NaN % rows w/ any NaN: 0.0\n",
      "Test NaN % rows w/ any NaN: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Build model-ready DataFrames (X_train_model / X_test_model)\n",
    "\n",
    "X_train_model = pd.DataFrame(\n",
    "    X_train_arr,\n",
    "    columns=final_feature_names,\n",
    "    index=X_train_raw.index,\n",
    ")\n",
    "X_train_model.insert(0, ID_COL, id_train)\n",
    "X_train_model[TARGET_COL] = y.values\n",
    "\n",
    "# Test\n",
    "X_test_model = pd.DataFrame(\n",
    "    X_test_arr,\n",
    "    columns=final_feature_names,\n",
    "    index=X_test_raw.index,\n",
    ")\n",
    "X_test_model.insert(0, ID_COL, id_test)\n",
    "\n",
    "print(\"X_train_model shape:\", X_train_model.shape)\n",
    "print(\"X_test_model shape:\", X_test_model.shape)\n",
    "\n",
    "print(\n",
    "    \"Train NaN % rows w/ any NaN:\",\n",
    "    np.mean(X_train_model.isna().any(axis=1)),\n",
    ")\n",
    "print(\n",
    "    \"Test NaN % rows w/ any NaN:\",\n",
    "    np.mean(X_test_model.isna().any(axis=1)),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d3839d7-8046-444f-be5c-5b7b97a30610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/domnjue/Desktop/Data-Science/projects/home_credit_default_risk/data/clean/model_train.csv\n",
      "Saved /Users/domnjue/Desktop/Data-Science/projects/home_credit_default_risk/data/clean/model_test.csv\n",
      "Saved feature names to /Users/domnjue/Desktop/Data-Science/projects/home_credit_default_risk/model/feature_names_raw_pipeline.pkl\n",
      "Saved preprocessor to /Users/domnjue/Desktop/Data-Science/projects/home_credit_default_risk/model/preprocessor.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save model matrices and preprocessing artifacts\n",
    "model_train_path = CLEAN_DIR / \"model_train.csv\"\n",
    "model_test_path  = CLEAN_DIR / \"model_test.csv\"\n",
    "\n",
    "X_train_model.to_csv(model_train_path, index=False)\n",
    "X_test_model.to_csv(model_test_path, index=False)\n",
    "\n",
    "print(f\"Saved {model_train_path}\")\n",
    "print(f\"Saved {model_test_path}\")\n",
    "\n",
    "# Also save feature names for deployment\n",
    "feature_names_path = MODEL_DIR / \"feature_names_raw_pipeline.pkl\"\n",
    "joblib.dump(final_feature_names, feature_names_path)\n",
    "print(f\"Saved feature names to {feature_names_path}\")\n",
    "\n",
    "# And save the sklearn preprocessor so the API can reuse identical logic\n",
    "preprocessor_path = MODEL_DIR / \"preprocessor.pkl\"\n",
    "joblib.dump(preprocessor, preprocessor_path)\n",
    "print(f\"Saved preprocessor to {preprocessor_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ad645d-1011-445c-9d56-ab7180f29627",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (project_venv)",
   "language": "python",
   "name": "project_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
