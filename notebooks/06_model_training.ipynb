{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5daab015",
   "metadata": {},
   "source": [
    "# 06_model_train\n",
    "\n",
    "Goal:\n",
    "\n",
    "- Load model-ready datasets\n",
    "- Train a LightGBM model with class weighting for imbalance\n",
    "- Evaluate ROC-AUC on a validation split\n",
    "- Plot ROC curve and feature importance\n",
    "- Train final model on full data and generate test predictions\n",
    "\n",
    "Inputs:\n",
    "- `data/clean/model_train.csv`  (has TARGET)\n",
    "- `data/clean/model_test.csv`   (same features, no TARGET)\n",
    "\n",
    "Outputs:\n",
    "- `data/clean/oof_val_preds.csv`      validation predictions (SK_ID_CURR, TARGET, PRED_PROB)\n",
    "- `data/clean/test_predictions.csv`   test predictions (SK_ID_CURR, PRED_PROB)\n",
    "\n",
    "Metric:\n",
    "- ROC-AUC on validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86150762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lightgbm as lgb  # pip install lightgbm\n",
    "\n",
    "CLEAN_DIR = Path(\"data/clean\")\n",
    "OUT_VAL_PATH = CLEAN_DIR / \"oof_val_preds.csv\"\n",
    "OUT_TEST_PATH = CLEAN_DIR / \"test_predictions.csv\"\n",
    "\n",
    "train_df = pd.read_csv(CLEAN_DIR / \"model_train.csv\")\n",
    "test_df  = pd.read_csv(CLEAN_DIR / \"model_test.csv\")\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "print(\"Default rate in train (TARGET==1):\", train_df[\"TARGET\"].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d7ce84",
   "metadata": {},
   "source": [
    "## 1. Prepare features / labels / ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b58acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COL = \"TARGET\"\n",
    "ID_COL = \"SK_ID_CURR\"\n",
    "\n",
    "# target\n",
    "y = train_df[TARGET_COL].astype(int).values\n",
    "\n",
    "# copy feature frames\n",
    "X = train_df.drop(columns=[TARGET_COL]).copy()\n",
    "X_test_final = test_df.copy()\n",
    "\n",
    "# save ids separately\n",
    "id_train = X[ID_COL].values\n",
    "id_test  = X_test_final[ID_COL].values\n",
    "\n",
    "# remove ID column from model features\n",
    "X = X.drop(columns=[ID_COL])\n",
    "X_test_final = X_test_final.drop(columns=[ID_COL])\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"X_test_final shape:\", X_test_final.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26499876",
   "metadata": {},
   "source": [
    "## 2. Train/validation split (stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c580c106",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_val, y_tr, y_val, id_tr, id_val = train_test_split(\n",
    "    X, y, id_train,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train split shape:\", X_tr.shape)\n",
    "print(\"Valid split shape:\", X_val.shape)\n",
    "print(\"Validation default rate:\", y_val.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e935bfee",
   "metadata": {},
   "source": [
    "## 3. LightGBM training with imbalance handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0d55eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap data for LightGBM\n",
    "lgb_train = lgb.Dataset(X_tr, label=y_tr)\n",
    "lgb_valid = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "# scale_pos_weight ~ (#neg / #pos)\n",
    "pos_weight = (y_tr==0).sum() / max((y_tr==1).sum(), 1)\n",
    "print(\"scale_pos_weight =\", pos_weight)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"num_leaves\": 64,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 5,\n",
    "    \"min_data_in_leaf\": 50,\n",
    "    \"scale_pos_weight\": pos_weight,\n",
    "    \"verbose\": -1,\n",
    "}\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    valid_sets=[lgb_train, lgb_valid],\n",
    "    valid_names=[\"train\",\"valid\"],\n",
    "    num_boost_round=5000,\n",
    "    early_stopping_rounds=200,\n",
    "    verbose_eval=100\n",
    ")\n",
    "\n",
    "print(\"Best iteration:\", model.best_iteration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf7abf7",
   "metadata": {},
   "source": [
    "## 4. Validation performance (ROC-AUC + ROC curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a844379",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred_proba = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "val_auc = roc_auc_score(y_val, val_pred_proba)\n",
    "print(\"Validation ROC-AUC:\", val_auc)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_val, val_pred_proba)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0,1],[0,1],\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(f\"ROC Curve (AUC={val_auc:.4f})\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be30344a",
   "metadata": {},
   "source": [
    "## 5. Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d27a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.feature_importance(importance_type=\"gain\")\n",
    "feat_imp = (\n",
    "    pd.DataFrame({\n",
    "        \"feature\": X.columns,\n",
    "        \"importance_gain\": importances\n",
    "    })\n",
    "    .sort_values(\"importance_gain\", ascending=False)\n",
    ")\n",
    "\n",
    "print(feat_imp.head(20))\n",
    "\n",
    "plt.figure(figsize=(6,8))\n",
    "feat_imp.head(25).sort_values(\"importance_gain\").plot(\n",
    "    kind=\"barh\",\n",
    "    x=\"feature\",\n",
    "    y=\"importance_gain\"\n",
    ")\n",
    "plt.title(\"Top 25 Feature Importances (gain)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bc6dfa",
   "metadata": {},
   "source": [
    "## 6. Save validation predictions for analysis / threshold tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f198c02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_DIR = Path(\"data/clean\")\n",
    "OUT_VAL_PATH = CLEAN_DIR / \"oof_val_preds.csv\"\n",
    "\n",
    "val_results = pd.DataFrame({\n",
    "    \"SK_ID_CURR\": id_val,\n",
    "    \"TARGET\": y_val,\n",
    "    \"PRED_PROB\": val_pred_proba\n",
    "})\n",
    "val_results.to_csv(OUT_VAL_PATH, index=False)\n",
    "print(\"Wrote validation preds to:\", OUT_VAL_PATH)\n",
    "\n",
    "val_results.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff28258",
   "metadata": {},
   "source": [
    "## 7. Final model on FULL data + test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0951d9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train final model using all rows\n",
    "full_lgb_train = lgb.Dataset(X, label=y)\n",
    "\n",
    "final_model = lgb.train(\n",
    "    {**params, \"verbose\": -1},\n",
    "    full_lgb_train,\n",
    "    num_boost_round=model.best_iteration  # reuse best iteration from earlier fit\n",
    ")\n",
    "\n",
    "# predict on test\n",
    "test_pred_proba = final_model.predict(X_test_final)\n",
    "\n",
    "submission_like = pd.DataFrame({\n",
    "    \"SK_ID_CURR\": id_test,\n",
    "    \"PRED_PROB\": test_pred_proba\n",
    "})\n",
    "\n",
    "OUT_TEST_PATH = CLEAN_DIR / \"test_predictions.csv\"\n",
    "submission_like.to_csv(OUT_TEST_PATH, index=False)\n",
    "\n",
    "print(\"Wrote test predictions to:\", OUT_TEST_PATH)\n",
    "submission_like.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
